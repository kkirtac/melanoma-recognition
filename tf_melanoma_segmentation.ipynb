{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-melanoma-segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/kkirtac/melanoma-recognition/blob/colab/tf_melanoma_segmentation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HpxCu0TEy8wG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "8620bfe1-be3c-4cbd-d052-37b566dc928b"
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.5)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.7)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 12.6 GB  I Proc size: 141.2 MB\n",
            "GPU RAM Free: 11439MB | Used: 0MB | Util   0% | Total 11439MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v0GUFMCWxsPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af3eb84c-ccb6-4413-a4a9-5bf295e483a7"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "fMnMpcCZlaD_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW_PfH9jDKHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cab3de47-67ad-41c9-9c8f-51f053017f28"
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\r\n",
            "drwxr-xr-x 2 root root 4096 Aug 31 20:42 sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uazfgki-FdmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "45061865-f3be-4824-8f31-f762f11a95ec"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Counting objects: 21590, done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 21590 (delta 0), reused 0 (delta 0), pack-reused 21584\u001b[K\n",
            "Receiving objects: 100% (21590/21590), 558.42 MiB | 15.85 MiB/s, done.\n",
            "Resolving deltas: 100% (12731/12731), done.\n",
            "Checking out files: 100% (2670/2670), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IvLcWfoFDhYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os,sys\n",
        "sys.path.append(\"/content/models/research/slim\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CyylG-8tDwA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a67462e8-cb0b-406a-e05d-eea4ffc0c32b"
      },
      "cell_type": "code",
      "source": [
        "from datasets import dataset_utils\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\"\n",
        "\n",
        "# download and extract the pre-trained weights in current dir\n",
        "dataset_utils.download_and_uncompress_tarball(url, './')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading resnet_v1_50_2016_08_28.tar.gz 100.0%\n",
            "Successfully downloaded resnet_v1_50_2016_08_28.tar.gz 95073259 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CWHlfiRzF33A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# move the extracted weights to log dir\n",
        "!mkdir /tmp/log\n",
        "!mv resnet_v1_50.ckpt /tmp/log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Py71SCUj6rn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2162c21a-731a-4a98-dd68-d32f46964dbd"
      },
      "cell_type": "code",
      "source": [
        "!ls -l /tmp/log"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 100100\r\n",
            "-rw-r----- 1 77690 5000 102500712 Aug 29  2016 resnet_v1_50.ckpt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GMS3cTkc5fOR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf resnet_v1_50_2016_08_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQerPziK3n7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a530179-bc01-40ee-f6ba-c1c337094608"
      },
      "cell_type": "code",
      "source": [
        "# download training mean file\n",
        "# mean_rgb_train_v8.npy\n",
        "mean_file_id = '1JpN9LsAMiqCeehL9NtW4RQL7fLTM6y36'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': mean_file_id})\n",
        "\n",
        "print(downloaded[\"title\"], downloaded[\"mimeType\"])\n",
        "\n",
        "downloaded.GetContentFile(downloaded['title'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_rgb_train_v8.npy application/octet-stream\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RDUXKQJgFGc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62ae579b-b102-4d78-b6e4-2e4a6ac6393a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(np.load('mean_rgb_train_v8.npy'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[184.58856828 153.699917   138.66204887]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5GrxVLIAzg3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72ee6eb6-38a7-48b5-f899-bee7c1594cbd"
      },
      "cell_type": "code",
      "source": [
        "# download training and validation data files\n",
        "\n",
        "# melanoma_val_224.tfrecords\n",
        "# validation_file_id = '1jauHXWlNVYc4gPoHU22fNTrcg17BQabS' \n",
        "\n",
        "# melanoma_val_v8.tfrecords\n",
        "validation_file_id = '1HAm1Y_N5gD1MN3wXi2GQglw3gste4tT7' \n",
        "\n",
        "downloaded = drive.CreateFile({'id': validation_file_id})\n",
        "\n",
        "print(downloaded[\"title\"], downloaded[\"mimeType\"])\n",
        "\n",
        "downloaded.GetContentFile(downloaded['title'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "melanoma_val_v8.tfrecords application/octet-stream\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Gr3kGa2OuDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a1a218f-4192-4a59-b336-1f3e67febc48"
      },
      "cell_type": "code",
      "source": [
        "# melanoma_train_224.tfrecords\n",
        "#training_file_id = '1CtxdJJ9uUv4wOtg5Lmx0o5vFtP4ovTrP'\n",
        "\n",
        "# melanoma_train_v8.tfrecords\n",
        "training_file_id = '1qt-dJsO8rZTQSMuCpt0BZb40gGvMcoDO'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': training_file_id})\n",
        "\n",
        "print(downloaded[\"title\"], downloaded[\"mimeType\"])\n",
        "\n",
        "downloaded.GetContentFile(downloaded['title'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "melanoma_train_v8.tfrecords application/octet-stream\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DgbMxU70B8LX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa0452b5-2386-4c28-bb6a-78751577012d"
      },
      "cell_type": "code",
      "source": [
        "# melanoma_test_v8.tfrecords\n",
        "test_file_id = '1b6ANEBvQXYZ_LJ89ck-jVLgjfyG8Fhqg'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': test_file_id})\n",
        "\n",
        "print(downloaded[\"title\"], downloaded[\"mimeType\"])\n",
        "\n",
        "downloaded.GetContentFile(downloaded['title'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "melanoma_test_v8.tfrecords application/octet-stream\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EndbFYDDz35R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mxvaoViT0DFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "f15c3842-c4b6-4374-eb57-b8df24ac9d1c"
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-09-04 10:59:41--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.239.63.98, 34.224.230.241, 34.231.150.116, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.239.63.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: â€˜ngrok-stable-linux-amd64.zipâ€™\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  3.48MB/s    in 1.5s    \n",
            "\n",
            "2018-09-04 10:59:43 (3.48 MB/s) - â€˜ngrok-stable-linux-amd64.zipâ€™ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JMoi7kKn0MyW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gCTohYa20Peg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23e08319-3b43-4f5a-8539-53c5efb6275d"
      },
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://325c0859.ngrok.io\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WfK5_EPPi6CC",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow.contrib.slim.nets import resnet_v1\n",
        "import datetime, os, glob, errno\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "\n",
        "\n",
        "def parser(record):\n",
        "  \"\"\"Parses input records and returns batch_size samples\n",
        "  \"\"\"\n",
        "  \n",
        "  # each tf.Example object in input .tfrecords file packs image and label as,\n",
        "  # dictionary with key: 'image', its value: a png-encoded bytearray representation\n",
        "  # dictionary with key: 'label', its value: a png-encoded bytearray representation,\n",
        "  # label is a binary image with 255 pixel value for foreground, 0 for background.\n",
        "  keys_to_features = {\n",
        "      \"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "      \"label\": tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "  }\n",
        "  \n",
        "  parsed = tf.parse_single_example(record, keys_to_features)\n",
        "  \n",
        "  # input image is a 3-channel RGB image\n",
        "  decoded_image = tf.image.decode_png(parsed[\"image\"], channels=3)\n",
        "  \n",
        "  # input label is a grayscale image\n",
        "  decoded_label = tf.image.decode_png(parsed[\"label\"], channels=1)\n",
        "   \n",
        "  # turn the label to floating-point with [0,1) range.\n",
        "  decoded_label = tf.image.convert_image_dtype(decoded_label, tf.float32)\n",
        "  #decoded_label = tf.round(decoded_label)\n",
        "  decoded_label = tf.to_int32(decoded_label)\n",
        "  \n",
        "  # drop the last dimension to get shape (H,W) instead of (H,W,1)\n",
        "  #decoded_label = tf.squeeze(decoded_label, axis=[2])\n",
        "  \n",
        "  # repeat along the last dimension\n",
        "  decoded_label = tf.concat([decoded_label, 1-decoded_label], axis=2) \n",
        "  ch0, ch1 = tf.split(value=decoded_label, num_or_size_splits=2, axis=2)\n",
        "  decoded_label = tf.concat(values=[ch1, ch0], axis=2)\n",
        "  \n",
        "  # per-channel mean values are taken from:\n",
        "  # https://github.com/yulequan/melanoma-recognition/blob/master/segmentation/ResNet-50-f8s-skin-train-val.prototxt\n",
        "  mean = tf.constant([184., 153., 138.],\n",
        "                     dtype=tf.float32, shape=[1, 1, 3], name='img_mean')\n",
        "  \n",
        "  # center the image with per-channel RGB mean\n",
        "  decoded_image = tf.to_float(decoded_image)\n",
        "  im_centered = decoded_image - mean\n",
        "  #return (im_centered, decoded_label)\n",
        "  \n",
        "  #im_centered = tf.image.per_image_standardization(decoded_image)\n",
        "        \n",
        "  imagedict = {'image': im_centered}\n",
        "  labeldict = {'label': decoded_label}\n",
        "  \n",
        "  return im_centered, decoded_label \n",
        "\n",
        "      \n",
        "\n",
        "def preproc_train(sample, target):\n",
        "  \n",
        "  seed = np.random.randint(0, 2**32)\n",
        "  \n",
        "  cropsize_img = [480, 480, 3]\n",
        "  cropsize_label = [480, 480, 2]\n",
        "  \n",
        "  image = tf.random_crop(sample, cropsize_img, seed=seed, name='crop_mirror_train_img')\n",
        "  label = tf.random_crop(target, cropsize_label, seed=seed, name='crop_mirror_train_label')\n",
        "  \n",
        "  tf.summary.image('crop_mirror_train_img', image)\n",
        "  tf.summary.image('crop_mirror_train_label', tf.cast(label,dtype=tf.uint8))\n",
        "  \n",
        "  image = tf.image.random_flip_left_right(image, seed=seed)\n",
        "  label =  tf.image.random_flip_left_right(label, seed=seed)\n",
        "\n",
        "  return image, label\n",
        "\n",
        "\n",
        "def preproc_val(sample, target):\n",
        "  \n",
        "  seed = np.random.randint(0, 2**32)\n",
        "  \n",
        "  image = tf.image.resize_image_with_crop_or_pad(sample, 480, 480)\n",
        "  label = tf.image.resize_image_with_crop_or_pad(target, 480, 480)  \n",
        "  \n",
        "  image = tf.image.random_flip_left_right(image, seed=seed)\n",
        "  label =  tf.image.random_flip_left_right(label, seed=seed)\n",
        "\n",
        "  return image, label\n",
        "\n",
        "  \n",
        "def my_input_fn_train(filename, batch_size, epochs):\n",
        "  \"\"\" reads the .tfrecords file,\n",
        "  process data using parser method,\n",
        "  shuffles data and returns the next batch.\n",
        "  \n",
        "  instantiates a tf.Dataset object and obtains a tf.data.Iterator, \n",
        "  that throws outOfRangeError when next batch goes out of the given epoch limit.\n",
        "  more info: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset([filename])\n",
        "  augmented = dataset.map(parser\n",
        "  ).map(\n",
        "      preproc_train\n",
        "  ).shuffle(\n",
        "      buffer_size=1000\n",
        "  ).batch(\n",
        "      batch_size\n",
        "  ).repeat(\n",
        "      epochs\n",
        "  )\n",
        "  \n",
        "  # this returns the next batch from the parser function\n",
        "  # the returned tuple is a dictionary for image and a dictionary for labels\n",
        "  features, labels = augmented.make_one_shot_iterator().get_next()\n",
        "  return {'image': features}, {'label': labels} \n",
        "\n",
        "\n",
        "def my_input_fn_val(filename, batch_size, epochs):\n",
        "  \"\"\" reads the .tfrecords file,\n",
        "  process data using parser method,\n",
        "  shuffles data and returns the next batch.\n",
        "  \n",
        "  instantiates a tf.Dataset object and obtains a tf.data.Iterator, \n",
        "  that throws outOfRangeError when next batch goes out of the given epoch limit.\n",
        "  more info: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "  \"\"\"\n",
        "  \n",
        "  dataset = tf.data.TFRecordDataset([filename])\n",
        "  augmented = dataset.map(parser\n",
        "  ).map(\n",
        "      preproc_val\n",
        "  ).batch(\n",
        "      batch_size\n",
        "  ).repeat(\n",
        "      epochs\n",
        "  )\n",
        "  \n",
        "  # this returns the next batch from the parser function\n",
        "  # the returned tuple is a dictionary for image and a dictionary for labels\n",
        "  features, labels = augmented.make_one_shot_iterator().get_next()\n",
        "  return {'image': features}, {'label': labels} \n",
        "          \n",
        "          \n",
        "    \n",
        "def get_deconv_filter(f_shape):\n",
        "  \"\"\"initialize a bilinear interpolation filter with\n",
        "  given shape.\n",
        "  \n",
        "  this filter values are trained and updated in the current computation graph.\n",
        "  \"\"\"\n",
        "  width = f_shape[0]\n",
        "  heigh = f_shape[0]\n",
        "  f = np.ceil(width/2.0)\n",
        "  c = (2 * f - 1 - f % 2) / (2.0 * f)\n",
        "  bilinear = np.zeros([f_shape[0], f_shape[1]])\n",
        "  for x in range(width):\n",
        "    for y in range(heigh):\n",
        "      value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n",
        "      bilinear[x, y] = value\n",
        "  weights = np.zeros(f_shape)\n",
        "  for i in range(f_shape[2]):\n",
        "    weights[:, :, i, i] = bilinear\n",
        "\n",
        "  init = tf.constant_initializer(value=weights,\n",
        "                                 dtype=tf.float32)\n",
        "  var = tf.get_variable(name=\"up_filter\",\n",
        "                        initializer=init,\n",
        "                        shape=weights.shape,\n",
        "                        regularizer=tf.contrib.layers.l2_regularizer(5e-4))\n",
        "  return var\n",
        "\n",
        "\n",
        "\n",
        "def upscore_layer(x, shape, num_classes, name, ksize, stride):\n",
        "  \"\"\"transposed convolution filter to learn upsampling filter values.\n",
        "  Given a feature map x, initialize a bilinear interpolation filter with\n",
        "  given shape to upsample the map based on the given stride.\n",
        "  \n",
        "  this filter values are trained and updated in the current computation graph.\n",
        "  \"\"\"   \n",
        "  strides = [1, stride, stride, 1]\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "    in_features = x.get_shape()[3].value\n",
        "    if shape is None:\n",
        "      in_shape = tf.shape(x)\n",
        "      h = ((in_shape[1] - 1) * stride) + 1\n",
        "      w = ((in_shape[2] - 1) * stride) + 1\n",
        "      new_shape = [in_shape[0], h, w, num_classes]\n",
        "    else:\n",
        "      new_shape = [shape[0], shape[1], shape[2], num_classes]\n",
        "    \n",
        "    output_shape = tf.stack(new_shape)\n",
        "    f_shape = [ksize, ksize, num_classes, in_features]\n",
        "    \n",
        "    weights = get_deconv_filter(f_shape)\n",
        "\n",
        "    #grp1 = tf.nn.conv2d_transpose(x[:, :, :, :int(in_features/2)], weights[:, :, 0, :], tf.stack([shape[0], shape[1], shape[2], 1]), strides = strides, padding='SAME')\n",
        "    #grp2 = tf.nn.conv2d_transpose(x[:, :, :, int(in_features/2):], weights[:, :, 1, :], tf.stack([shape[0], shape[1], shape[2], 1]), strides = strides, padding='SAME')\n",
        "    \n",
        "    #deconv = tf.concat(axis=3, values=[grp1, grp2])\n",
        "    \n",
        "    deconv = tf.nn.conv2d_transpose(x, weights, output_shape, strides = strides, padding='SAME')\n",
        "    \n",
        "    return deconv\n",
        "\n",
        "  \n",
        "  \n",
        "def score_layer(x, name, num_classes, stddev = 0.001): \n",
        "  \"\"\"receives a feature map and trains a linear scoring filter Wx+b\n",
        "  \n",
        "  this result with a new feature map with a consistent shape to be fused (per-pixel addition)\n",
        "  with the corresponding upsampling result from the same input feature map x.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE) as scope:\n",
        "    # get number of input channels\n",
        "    in_features = x.get_shape()[3].value\n",
        "    shape = [1, 1, in_features, num_classes]\n",
        "    w_decay = 5e-4\n",
        "    #init = tf.truncated_normal_initializer(stddev = stddev)\n",
        "    init = tf.constant_initializer(0.0)\n",
        "    weights = tf.get_variable(\"weights\", \n",
        "                              shape = shape, \n",
        "                              initializer = init,\n",
        "                              regularizer=tf.contrib.layers.l2_regularizer(w_decay))\n",
        "    #collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n",
        "\n",
        "    #if not tf.get_variable_scope().reuse:\n",
        "    #  weight_decay = tf.multiply(tf.nn.l2_loss(weights), w_decay, name='weight_loss')\n",
        "    #  tf.add_to_collection(collection_name, weight_decay)\n",
        "\n",
        "    conv = tf.nn.conv2d(x, weights, [1, 1, 1, 1], padding='SAME')\n",
        "    \n",
        "    # Apply bias\n",
        "    initializer = tf.constant_initializer(0.0)\n",
        "    conv_biases = tf.get_variable(name='biases', shape=[num_classes], initializer = initializer)\n",
        "    bias = tf.nn.bias_add(conv, conv_biases)\n",
        "    \n",
        "    return bias     \n",
        "      \n",
        "  \n",
        "\n",
        "def resnet_v1_50_fcn(features, labels, mode, params):\n",
        "    \n",
        "    num_classes = params['num_classes']\n",
        "   \n",
        "    \n",
        "    # output_stride denotes the rate of downsampling in the resnet network,i.e.,\n",
        "    # 32 results with a feature map with a 1/32 downsampling rate such as,\n",
        "    # (224,224) input is decreased to (8,8) resolution. \n",
        "    # Then, the rest of the code starts upsampling from this output, until reaching\n",
        "    # the input resolution.\n",
        "    with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=0.0005)):\n",
        "      net, end_points = resnet_v1.resnet_v1_50(features['image'],\n",
        "                                             global_pool=False,\n",
        "                                             output_stride=32)\n",
        "    \n",
        "      scale5 = net\n",
        "      scale4 = end_points['resnet_v1_50/block3/unit_5/bottleneck_v1']\n",
        "      scale3 = end_points['resnet_v1_50/block2/unit_3/bottleneck_v1']\n",
        "      scale2 = end_points['resnet_v1_50/block1/unit_2/bottleneck_v1']\n",
        "      input_x = features['image']\n",
        "    \n",
        "    with tf.variable_scope('scale_fcn'):\n",
        "      \n",
        "      score_scale5 = score_layer(scale5, \"score_scale5\", num_classes = num_classes)\n",
        "      upscore2 = upscore_layer(score_scale5, shape = tf.shape(scale4), num_classes = num_classes, name = \"upscore2\", ksize = 4, stride = 2) \n",
        "      \n",
        "      score_scale4 = score_layer(scale4, \"score_scale4\", num_classes = num_classes)\n",
        "      fuse_scale4 = tf.add(upscore2, score_scale4)\n",
        "      \n",
        "      upscore4 = upscore_layer(fuse_scale4, shape = tf.shape(scale3), num_classes = num_classes, name = \"upscore4\", ksize = 4, stride = 2) \n",
        "      score_scale3 = score_layer(scale3, \"score_scale3\", num_classes = num_classes)\n",
        "      fuse_scale3 = tf.add(upscore4, score_scale3)\n",
        "      \n",
        "      upscore32 = upscore_layer(fuse_scale3, shape = tf.shape(input_x), num_classes = num_classes, name = \"upscore32\", ksize = 16, stride = 8)\n",
        "\n",
        "      pred_up = tf.argmax(upscore32, axis = 3)   # shape: [4 480 480]\n",
        "      pred = tf.expand_dims(pred_up, dim = 3, name='pred')  # shape: [4 480 480 1]  \n",
        "      \n",
        "#       sess = tf.Session()\n",
        "#       sess.run(tf.global_variables_initializer())\n",
        "#       sess.run(tf.local_variables_initializer())\n",
        "#       print(sess.run(tf.shape(scale5)))\n",
        "#       print(sess.run(tf.shape(upscore32)))\n",
        "#       print(sess.run(tf.shape(pred_up)))\n",
        "#       print(sess.run(tf.shape(pred)))\n",
        "#       sess.close()\n",
        "      \n",
        "    tf.summary.image('ground-truth', tf.expand_dims(255.0*tf.to_float(tf.argmax(labels['label'], axis = 3)), axis = 3))\n",
        "    tf.summary.image('prediction', 255.0*tf.to_float(pred))\n",
        "    \n",
        " \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=pred_up)\n",
        "    \n",
        "    # Using tf.losses, any loss is added to the tf.GraphKeys.LOSSES collection\n",
        "    # We can then call the total loss easily    \n",
        "    tf.losses.softmax_cross_entropy(onehot_labels=labels['label'], \n",
        "                                    logits=upscore32)\n",
        "    loss = tf.losses.get_total_loss() \n",
        "    tf.summary.scalar('loss', loss)\n",
        "    \n",
        "\n",
        "    ## Compute evaluation metrics.\n",
        "    # compute mean intersection over union, which corresponds to JA metric in the paper.\n",
        "    iou_op, update_op = tf.metrics.mean_iou(\n",
        "      labels=tf.reshape(tf.argmax(labels['label'], axis = 3), [-1]),\n",
        "      num_classes=2,\n",
        "      predictions=tf.to_int32(tf.reshape(pred_up, [-1])), name=\"acc_op\")\n",
        "  \n",
        "    with tf.control_dependencies([update_op]):\n",
        "      iou = tf.identity(iou_op)\n",
        "      \n",
        "    tf.summary.scalar('iou', iou)   \n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode, loss=loss, eval_metric_ops={'iou': (iou_op, update_op)})\n",
        "    \n",
        "    # Create training op.\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "    \n",
        "    \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    \n",
        "    training_loss = tf.identity(loss, name='training_loss')\n",
        "    \n",
        "    # set initial learning rate and \n",
        "    # scale it by 0.1 in every 3000 steps\n",
        "    # as stated in https://github.com/yulequan/melanoma-recognition/blob/master/segmentation/solver.prototxt\n",
        "    boundaries = [i*params['decay_steps'] for i in range(1, int(np.ceil(params['max_steps']/params['decay_steps']))) ]\n",
        "    boundaries[-1] = params['max_steps']\n",
        "    initial = [params['initial_learning_rate']]\n",
        "    values = initial + [params['initial_learning_rate']*(0.1**i) for i in range(1,len(boundaries)+1)]\n",
        "    #lr = tf.train.piecewise_constant(global_step, boundaries, values)\n",
        "    lr = tf.constant(params['initial_learning_rate'])\n",
        "    \n",
        "    tf.summary.scalar('lr', lr)\n",
        "    \n",
        "    # Variables that affect learning rate\n",
        "    var_resnet_batchnorm = [var for var in tf.trainable_variables() \n",
        "                            if ('conv1/BatchNorm' in var.name or \n",
        "                                'conv2/BatchNorm' in var.name or \n",
        "                                'conv3/BatchNorm' in var.name or \n",
        "                                'shortcut/BatchNorm' in var.name)] \n",
        "  \n",
        "    var_upscale = [var for var in tf.trainable_variables() \n",
        "                   if 'score' in var.name and 'bias' not in var.name]\n",
        "    \n",
        "    var_score_bias = [var for var in tf.trainable_variables() \n",
        "                      if 'score' in var.name and 'bias' in var.name]\n",
        " \n",
        "    var_rest = [var for var in tf.trainable_variables() \n",
        "                if var not in var_resnet_batchnorm + var_upscale + var_score_bias]\n",
        "    \n",
        "    # this is as stated in the paper and prototxt file,\n",
        "    # https://github.com/yulequan/melanoma-recognition/blob/master/segmentation/ResNet-50-f8s-skin-train-val.prototxt\n",
        "    # batchnorm variables placed just after each convolutional layer in each resnet block, are not updated.\n",
        "    # so we set zero learning for these variables\n",
        "    #opt1 = tf.train.MomentumOptimizer(0, 0.9)\n",
        "  \n",
        "    # this is also due to the prototxt file and the paper,\n",
        "    # scoring and upsampling filters receive 0.1 * learning rate\n",
        "    # https://github.com/yulequan/melanoma-recognition/blob/master/segmentation/ResNet-50-f8s-skin-train-val.prototxt\n",
        "    opt_upscale = tf.train.MomentumOptimizer(lr*0.1, 0.9)\n",
        "  \n",
        "    # rest of the variables receive the current learning rate\n",
        "    opt_scorebias = tf.train.MomentumOptimizer(lr*0.2, 0.9)\n",
        "  \n",
        "    # rest of the variables receive the current learning rate\n",
        "    opt_rest = tf.train.MomentumOptimizer(lr, 0.9)\n",
        "  \n",
        "    # gradient op: obtain the gradients with loss and given variables\n",
        "    grads = tf.gradients(loss, var_upscale + var_score_bias + var_rest)\n",
        "  \n",
        "    # grads for the first set of variables, currently has no effect due to zero learning rate.\n",
        "    #grads1 = grads[:len(var_resnet_batchnorm)]\n",
        "  \n",
        "    # grads for scoring and upsampling filters. Will get updated based on 0.1*learning_rate\n",
        "    grads_upscale = grads[:len(var_upscale)]\n",
        "  \n",
        "    #for i,val in enumerate(grads2):\n",
        "    #  tf.summary.histogram('upscale_grads_{}'.format(i), val)\n",
        "  \n",
        "    # grads for bias variables\n",
        "    grads_scorebias = grads[len(var_upscale):len(var_upscale)+len(var_score_bias)]\n",
        "    \n",
        "    # grads for the rest of variables\n",
        "    grads_rest = grads[len(var_upscale)+len(var_score_bias):]\n",
        "    \n",
        "  \n",
        "    #train_op1 = opt1.apply_gradients(zip(grads1, var_resnet_batchnorm), global_step=global_step)\n",
        "    train_op_upscale = opt_upscale.apply_gradients(zip(grads_upscale, var_upscale), global_step=global_step)\n",
        "    train_op_scorebias = opt_scorebias.apply_gradients(zip(grads_scorebias, var_score_bias), global_step=global_step)\n",
        "    train_op_rest = opt_rest.apply_gradients(zip(grads_rest, var_rest), global_step=global_step)\n",
        "\n",
        "    train_op = tf.group(train_op_upscale, train_op_scorebias, train_op_rest)\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "  \n",
        "  \n",
        "class MyLoggingAverageLossHook(tf.train.LoggingTensorHook):\n",
        "  \n",
        "  def __init__(self, tensors, every_n_iter):\n",
        "    super().__init__(tensors=tensors, every_n_iter=every_n_iter)\n",
        "    \n",
        "    # keep track of previous losses\n",
        "    self.losses=[]\n",
        "    self.every_n_iter=every_n_iter\n",
        "    \n",
        "    \n",
        "  def after_run(self, run_context, run_values):\n",
        "    _ = run_context\n",
        "    \n",
        "    self._tag = ''\n",
        "    \n",
        "    # please put only one loss tensor\n",
        "    for tag in self._tag_order:\n",
        "      self.losses.append(run_values.results[tag])\n",
        "      self._tag = tag    \n",
        "    \n",
        "    if self._should_trigger:\n",
        "      self._log_tensors(run_values.results)\n",
        "\n",
        "    self._iter_count += 1\n",
        "    \n",
        "  \n",
        "  def _log_tensors(self, tensor_values):\n",
        "    \n",
        "    if self._iter_count % self.every_n_iter == 0:\n",
        "      original = np.get_printoptions()\n",
        "      np.set_printoptions(suppress=True)\n",
        "      logging.info(\"%s = %s\" % (self._tag, np.mean(self.losses)))      \n",
        "      np.set_printoptions(**original)\n",
        "      self.losses=[]\n",
        "  \n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "\n",
        "# filename_train = 'melanoma_train_224.tfrecords'\n",
        "# filename_val = 'melanoma_val_224.tfrecords'\n",
        "filename_train = 'melanoma_train_v8.tfrecords'\n",
        "filename_val = 'melanoma_val_v8.tfrecords'\n",
        "save_model_dir='/tmp/log'\n",
        "restore_ckpt_path='/tmp/log/resnet_v1_50.ckpt'\n",
        "save_checkpoints_steps=500\n",
        "params = {'initial_learning_rate' : 1e-3,\n",
        "          'num_classes' : 2,\n",
        "          'decay_steps' : 3000,\n",
        "          'max_steps' : 100000\n",
        "         }\n",
        "  \n",
        "  \n",
        "#with tf.Session() as sess:\n",
        "  \n",
        "# exclude_restore = [var.name for var in tf.global_variables() if not ('logits' in var.name or 'scale_fcn' in var.name or 'Momentum' in var.name) ] \n",
        "# variables_to_restore = slim.get_variables_to_restore(exclude=exclude_restore)\n",
        "# tf.train.init_from_checkpoint(restore_ckpt_path,\n",
        "#                                {v.name.split(':')[0]: v for v in variables_to_restore})\n",
        "  \n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  \n",
        "        \n",
        "run_config = tf.estimator.RunConfig(save_checkpoints_steps=save_checkpoints_steps)\n",
        "  \n",
        "  \n",
        "avg_train_loss_log = {\"average_training_loss\": \"training_loss\"}\n",
        "  \n",
        "  # this custom logging hook is created to average last every_n_iter loss values\n",
        "  # and display the result as an INFO\n",
        "my_averageloss_logging_hook = MyLoggingAverageLossHook(\n",
        "      tensors=avg_train_loss_log,\n",
        "      every_n_iter=10)\n",
        "        \n",
        "ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=restore_ckpt_path,\n",
        "                       vars_to_warm_start='.*resnet_v1.*')\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "            model_fn=resnet_v1_50_fcn,\n",
        "            model_dir=save_model_dir,\n",
        "            params=params,\n",
        "            config=run_config,\n",
        "            warm_start_from=ws\n",
        "        )\n",
        "\n",
        "print(estimator.eval_dir())\n",
        "\n",
        "print(os.listdir(save_model_dir))\n",
        "\n",
        "# train_spec = tf.estimator.TrainSpec(input_fn=lambda:my_input_fn(filename_train, batch_size=4, epochs=500), \n",
        "#                                       max_steps=params['max_steps'],\n",
        "#                                       hooks=[my_averageloss_logging_hook]) \n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(input_fn=lambda:my_input_fn_train(filename_train, batch_size=4, epochs=500), \n",
        "                                      max_steps=params['max_steps']) \n",
        "  \n",
        "eval_spec = tf.estimator.EvalSpec(input_fn=lambda:my_input_fn_val(filename_val, batch_size=4, epochs=1),\n",
        "                                 steps=None, throttle_secs=50)\n",
        "       \n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "  \n",
        "      \n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTavDcdfYePO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluate on the test set"
      ]
    },
    {
      "metadata": {
        "id": "d4kTl07FRbLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f14f12a7-3e6e-453f-e05e-91bfe4d05530"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "filename_test = 'melanoma_test_v8.tfrecords'\n",
        "\n",
        "save_model_dir='/tmp/log'\n",
        "params = {'initial_learning_rate' : 1e-3,\n",
        "          'num_classes' : 2,\n",
        "          'decay_steps' : 3000,\n",
        "          'max_steps' : 100000\n",
        "         }\n",
        "  \n",
        "  \n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  \n",
        "estimator = tf.estimator.Estimator(\n",
        "            model_fn=resnet_v1_50_fcn,\n",
        "            model_dir=save_model_dir,\n",
        "            params = params)\n",
        "  \n",
        "       \n",
        "estimator.evaluate(input_fn=lambda:my_input_fn_val(filename_test, batch_size=4, epochs=1))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/log', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3c7281f208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-09-04-04:28:05\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/log/model.ckpt-100002\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-09-04-04:29:36\n",
            "INFO:tensorflow:Saving dict for global step 100002: global_step = 100002, iou = 0.8242391, loss = 1.3000946\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100002: /tmp/log/model.ckpt-100002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'global_step': 100002, 'iou': 0.8242391, 'loss': 1.3000946}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "wjUGTcys29qg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf /tmp/log/model*\n",
        "# !rm -rf /tmp/log/events*\n",
        "# !rm -rf /tmp/log/graph*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RqCpNQrqIMjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf /tmp/log/checkpoint /tmp/log/eval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fuszi5kDIHNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b14af26-7142-47cb-c8a2-47243b840997"
      },
      "cell_type": "code",
      "source": [
        "!ls -la /tmp/log"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/tmp/log': No such file or directory\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QPKvbYGdajkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}