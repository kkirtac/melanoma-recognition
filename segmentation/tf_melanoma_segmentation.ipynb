{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-melanoma-segmentation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HpxCu0TEy8wG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMnMpcCZlaD_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dW_PfH9jDKHI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uazfgki-FdmO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvLcWfoFDhYL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os,sys\n",
        "sys.path.append(\"/content/models/research/slim\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CyylG-8tDwA4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from datasets import dataset_utils\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\"\n",
        "\n",
        "# Specify where you want to download the model to\n",
        "checkpoints_dir = '/content'\n",
        "\n",
        "if not tf.gfile.Exists(checkpoints_dir):\n",
        "    tf.gfile.MakeDirs(checkpoints_dir)\n",
        "\n",
        "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWHlfiRzF33A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf resnet_v1_50_2016_08_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6KbLUQW0udx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GrxVLIAzg3R",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d68b36e9-b674-4c2b-e696-3390d8fab221",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524299958344,
          "user_tz": -180,
          "elapsed": 2283,
          "user": {
            "displayName": "Kadir K",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110800347693042128479"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# download training and validation data files\n",
        "validation_file_id = '1sepSrVG4_mnnmjlB-SAPorN4ksUN8jsq' \n",
        "\n",
        "downloaded = drive.CreateFile({'id': validation_file_id})\n",
        "\n",
        "print(downloaded[\"title\"], downloaded[\"mimeType\"])\n",
        "\n",
        "downloaded.GetContentFile(downloaded['title'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "melanoma_val.tfrecords application/octet-stream\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Gr3kGa2OuDW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af90dc39-57da-4ffe-bce9-8ffa0df7001b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524299967324,
          "user_tz": -180,
          "elapsed": 8960,
          "user": {
            "displayName": "Kadir K",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110800347693042128479"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "training_file_id = '1inW8VfAfSYykAGqyarpigA37rUBX4C_w'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': training_file_id})\n",
        "\n",
        "print(downloaded[\"title\"], downloaded[\"mimeType\"])\n",
        "\n",
        "downloaded.GetContentFile(downloaded['title'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "melanoma_train.tfrecords application/octet-stream\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2EUzf3pk3rqg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "def parser(record):\n",
        "  \"\"\"Parses input records and returns batch_size samples\n",
        "  \"\"\"\n",
        "  keys_to_features = {\n",
        "      \"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "      \"label\": tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "  }\n",
        "  parsed = tf.parse_single_example(record, keys_to_features)\n",
        "  \n",
        "  decoded_image = tf.image.decode_png(parsed[\"image\"], channels=3)\n",
        "  \n",
        "  decoded_label = tf.image.decode_png(parsed[\"label\"], channels=1) \n",
        "  \n",
        "  decoded_image = tf.to_float(decoded_image)\n",
        "  decoded_label = tf.image.convert_image_dtype(decoded_label, tf.float32)\n",
        "  decoded_label = tf.to_int32(decoded_label)\n",
        "  \n",
        "  decoded_label = tf.squeeze(input=decoded_label, axis=[2]) # drop the last axis\n",
        "  \n",
        "  mean = tf.constant([182., 149., 135.],\n",
        "                     dtype=tf.float32, shape=[1, 1, 3], name='img_mean')\n",
        "  \n",
        "  im_centered = decoded_image - mean\n",
        "  \n",
        "  return (im_centered, decoded_label)\n",
        "\n",
        "#return {\"image\": im_centered,\n",
        "#       \"label\": decoded_label}\n",
        "\n",
        "          \n",
        "def my_input_fn(filename, batch_size, epochs):\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      [filename]\n",
        "  ).map(\n",
        "      parser\n",
        "  ).shuffle(\n",
        "      buffer_size=1000\n",
        "  ).batch(\n",
        "      batch_size\n",
        "  ).repeat(\n",
        "      epochs\n",
        "  )\n",
        "  return dataset.make_one_shot_iterator().get_next()\n",
        "      \n",
        "def get_deconv_filter(f_shape):\n",
        "    width = f_shape[0]\n",
        "    heigh = f_shape[0]\n",
        "    f = np.ceil(width/2.0)\n",
        "    c = (2 * f - 1 - f % 2) / (2.0 * f)\n",
        "    bilinear = np.zeros([f_shape[0], f_shape[1]])\n",
        "    for x in range(width):\n",
        "        for y in range(heigh):\n",
        "            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n",
        "            bilinear[x, y] = value\n",
        "    weights = np.zeros(f_shape)\n",
        "    for i in range(f_shape[2]):\n",
        "        weights[:, :, i, i] = bilinear\n",
        "\n",
        "    init = tf.constant_initializer(value=weights,\n",
        "                                   dtype=tf.float32)\n",
        "    var = tf.get_variable(name=\"up_filter\", initializer=init,\n",
        "                          shape=weights.shape)\n",
        "    return var\n",
        "\n",
        "def upscore_layer(x, shape, num_classes, name, ksize, stride):\n",
        "    strides = [1, stride, stride, 1]\n",
        "    with tf.variable_scope(name):\n",
        "        in_features = x.get_shape()[3].value\n",
        "        if shape is None:\n",
        "            in_shape = tf.shape(x)\n",
        "            h = ((in_shape[1] - 1) * stride) + 1\n",
        "            w = ((in_shape[2] - 1) * stride) + 1\n",
        "            new_shape = [in_shape[0], h, w, num_classes]\n",
        "        else:\n",
        "            new_shape = [shape[0], shape[1], shape[2], num_classes]\n",
        "        output_shape = tf.stack(new_shape)\n",
        "        f_shape = [ksize, ksize, num_classes, in_features]\n",
        "        num_input = ksize * ksize * in_features / stride\n",
        "        stddev = (2 / num_input)**0.5\n",
        "        weights = get_deconv_filter(f_shape)\n",
        "        deconv = tf.nn.conv2d_transpose(x, weights, output_shape, strides = strides, padding='SAME')\n",
        "        return deconv\n",
        "\n",
        "def score_layer(x, name, num_classes, stddev = 0.001): \n",
        "    with tf.variable_scope(name) as scope:\n",
        "        # get number of input channels\n",
        "        in_features = x.get_shape()[3].value\n",
        "        shape = [1, 1, in_features, num_classes]\n",
        "        w_decay = 5e-4\n",
        "        init = tf.truncated_normal_initializer(stddev = stddev)\n",
        "        weights = tf.get_variable(\"weights\", shape = shape, initializer = init)\n",
        "        collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n",
        "\n",
        "        if not tf.get_variable_scope().reuse:\n",
        "            weight_decay = tf.multiply(tf.nn.l2_loss(weights), w_decay, name='weight_loss')\n",
        "            tf.add_to_collection(collection_name, weight_decay)\n",
        "\n",
        "        conv = tf.nn.conv2d(x, weights, [1, 1, 1, 1], padding='SAME')\n",
        "        # Apply bias\n",
        "        initializer = tf.constant_initializer(0.0)\n",
        "        conv_biases = tf.get_variable(name='biases', shape=[num_classes],initializer=initializer)\n",
        "\n",
        "        bias = tf.nn.bias_add(conv, conv_biases)\n",
        "\n",
        "        return bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1mxyhwiT--y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size=1\n",
        "epochs=10\n",
        "initial_learning_rate=1e-3\n",
        "learning_rate_decay_factor=0.96\n",
        "num_classes=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2YYLSEASUlS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow.contrib.slim.nets import resnet_v1\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    \n",
        "    \n",
        "    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
        "      net, end_points = resnet_v1.resnet_v1_50(features,\n",
        "                                               global_pool=False, \n",
        "                                               output_stride=32)\n",
        "      \n",
        "      \n",
        "      scale5 = net\n",
        "      scale4 = end_points['resnet_v1_50/block3/unit_5/bottleneck_v1']\n",
        "      scale3 = end_points['resnet_v1_50/block2/unit_3/bottleneck_v1']\n",
        "      scale2 = end_points['resnet_v1_50/block1/unit_2/bottleneck_v1']\n",
        "      input_x = features\n",
        "            \n",
        "      # stride=32\n",
        "      #resnet_v1_50/block3/unit_5/bottleneck_v1 (1, 15, 15, 1024)\n",
        "      #net.shape: (1, 8, 8, 2048)  \n",
        "      #resnet_v1_50/block2/unit_3/bottleneck_v1 (1, 29, 29, 512)\n",
        "      #resnet_v1_50/block1/unit_2/bottleneck_v1 (1, 57, 57, 256)\n",
        "      #resnet_v1_50/conv1 (1, 113, 113, 64)\n",
        "        \n",
        "      \n",
        "    with tf.variable_scope('scale_fcn'):\n",
        "        upscore2 = upscore_layer(scale5, shape = tf.shape(scale4), num_classes = num_classes, name = \"upscore2\", ksize = 4, stride = 2) \n",
        "        score_scale4 = score_layer(scale4, \"score_scale4\", num_classes = num_classes)\n",
        "        fuse_scale4 = tf.add(upscore2, score_scale4)\n",
        "\n",
        "        upscore4 = upscore_layer(fuse_scale4, shape = tf.shape(scale3), num_classes = num_classes, name = \"upscore4\", ksize = 4, stride = 2) \n",
        "        score_scale3 = score_layer(scale3, \"score_scale3\", num_classes = num_classes)\n",
        "        fuse_scale3 = tf.add(upscore4, score_scale3)\n",
        "\n",
        "        upscore8 = upscore_layer(fuse_scale3, shape = tf.shape(scale2), num_classes = num_classes, name = \"upscore8\", ksize = 4, stride = 2) \n",
        "        #score_scale2 = score_layer(scale2, \"score_scale2\", num_classes = num_classes)\n",
        "        #fuse_scale2 = tf.add(upscore8, score_scale2)\n",
        "        #upscore32 = upscore_layer(fuse_scale2, shape = tf.shape(input_x), num_classes = num_classes, name = \"upscore32\", ksize = 8, stride = 4)\n",
        "        \n",
        "        upscore32 = upscore_layer(upscore8, shape = tf.shape(input_x), num_classes = num_classes, name = \"upscore32\", ksize = 8, stride = 4)\n",
        "        \n",
        "        pred_up = tf.argmax(upscore32, dimension = 3)\n",
        "        \n",
        "        pred = tf.expand_dims(pred_up, dim = 3, name='pred')\n",
        "\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        predictions = {\n",
        "            'probabilities': pred,\n",
        "            'logits': pred\n",
        "        }\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "    \n",
        "    \n",
        "    exclude = ['resnet_v1_50/logits', 'scale_fcn/score_scale2/biases', \n",
        "               'scale_fcn/score_scale2/weights', 'scale_fcn/score_scale3/biases',\n",
        "              'scale_fcn/score_scale3/weights', 'scale_fcn/score_scale4/biases',\n",
        "              'scale_fcn/score_scale4/weights', 'scale_fcn/upscore2/up_filter',\n",
        "              'scale_fcn/upscore4/up_filter', 'scale_fcn/upscore8/up_filter',\n",
        "              'scale_fcn/upscore32/up_filter']\n",
        "    variables_to_restore = slim.get_variables_to_restore(exclude=exclude)\n",
        "    tf.train.init_from_checkpoint(params['ckpt_path'],\n",
        "                                  {v.name.split(':')[0]: v for v in variables_to_restore})\n",
        "    \n",
        "    \n",
        "    # Using tf.losses, any loss is added to the tf.GraphKeys.LOSSES collection\n",
        "    # We can then call the total loss easily    \n",
        "    tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=upscore32)\n",
        "    loss = tf.losses.get_total_loss()\n",
        "    \n",
        "    # Compute evaluation metrics.\n",
        "\n",
        "    #accuracy = tf.metrics.mean_iou(labels=labels,\n",
        "     #                              num_classes=2,\n",
        "      #                             predictions=pred_up,\n",
        "       #                            name=\"acc_op\")\n",
        "      \n",
        "    metrics = {'accuracy': accuracy}\n",
        "    tf.summary.scalar('accuracy', accuracy[1])\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode, loss=loss, eval_metric_ops=metrics)\n",
        "    \n",
        "    # Create training op.\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "    \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    \n",
        "    # Variables that affect learning rate\n",
        "    decay_steps=10000\n",
        "\n",
        "    lr = tf.train.exponential_decay(initial_learning_rate,\n",
        "                                    global_step,\n",
        "                                    decay_steps,\n",
        "                                    learning_rate_decay_factor,\n",
        "                                    staircase=True)\n",
        "    \n",
        "    train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss, \n",
        "                                                              global_step=global_step) \n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NI-5vBDOiCIG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "    with tf.Session() as session:\n",
        "        \n",
        "        tf.logging.set_verbosity(tf.logging.INFO)\n",
        "        \n",
        "#         tensors_to_log = {'accuracy' :accuracy}\n",
        "        \n",
        "#         logging_tensor_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)       \n",
        "        \n",
        "        run_config = tf.estimator.RunConfig(save_checkpoints_steps=1000)\n",
        "        \n",
        "        estimator = tf.estimator.Estimator(\n",
        "            model_fn=model_fn,\n",
        "            model_dir='./chekpoints',\n",
        "            params={\n",
        "                'ckpt_path': './resnet_v1_50.ckpt'\n",
        "            },\n",
        "            config=run_config\n",
        "        )\n",
        "        \n",
        "        max_steps = 10000\n",
        "        \n",
        "        print({'max_steps': max_steps})\n",
        "        \n",
        "        train_spec = tf.estimator.TrainSpec(input_fn=lambda: my_input_fn('melanoma_train.tfrecords', batch_size, epochs), max_steps=max_steps) \n",
        "        eval_spec = tf.estimator.EvalSpec(input_fn=lambda: my_input_fn('melanoma_val.tfrecords', 1, 1), start_delay_secs=5, \n",
        "                                          steps=100,\n",
        "                                         throttle_secs=10)\n",
        "        \n",
        "        tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "            \n",
        "#         # Train the Model\n",
        "#         classifier.train(input_fn=lambda:train_input_fn())\n",
        "        \n",
        "#         classifier.evaluate(input_fn=lambda:val_input_fn())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WfK5_EPPi6CC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow.contrib.slim.nets import resnet_v1\n",
        "\n",
        "\n",
        "def parser(record):\n",
        "  \"\"\"Parses input records and returns batch_size samples\n",
        "  \"\"\"\n",
        "  \n",
        "  keys_to_features = {\n",
        "      \"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "      \"label\": tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
        "  }\n",
        "  \n",
        "  parsed = tf.parse_single_example(record, keys_to_features)\n",
        "  \n",
        "  decoded_image = tf.image.decode_png(parsed[\"image\"], channels=3)\n",
        "  #decoded_image = tf.image.resize_images(decoded_image, [224, 224])\n",
        "  \n",
        "  decoded_label = tf.image.decode_png(parsed[\"label\"], channels=1)   \n",
        "  #decoded_label = tf.image.resize_images(decoded_label, [224, 224])\n",
        "  \n",
        "  decoded_image = tf.to_float(decoded_image)\n",
        "  decoded_label = tf.image.convert_image_dtype(decoded_label, tf.float32)\n",
        "  decoded_label = tf.to_int32(decoded_label)\n",
        "  \n",
        "  decoded_label = tf.squeeze(input=decoded_label, axis=[2]) # drop the last axis\n",
        "  \n",
        "  mean = tf.constant([182., 149., 135.],\n",
        "                     dtype=tf.float32, shape=[1, 1, 3], name='img_mean')\n",
        "  \n",
        "  im_centered = decoded_image - mean\n",
        "  return (im_centered, decoded_label)\n",
        "        \n",
        "        #return {\"image\": im_centered,\n",
        "         #       \"label\": decoded_label}\n",
        "      \n",
        "      \n",
        "def my_input_fn(filename, batch_size, epochs):\n",
        "  \n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      [filename]\n",
        "  ).map(\n",
        "      parser\n",
        "  ).shuffle(\n",
        "      buffer_size=1000\n",
        "  ).batch(\n",
        "      batch_size\n",
        "  ).repeat(\n",
        "      epochs\n",
        "  )\n",
        "  \n",
        "  return dataset.make_one_shot_iterator().get_next() \n",
        "          \n",
        "          \n",
        "def get_deconv_filter(f_shape):\n",
        "    width = f_shape[0]\n",
        "    heigh = f_shape[0]\n",
        "    f = np.ceil(width/2.0)\n",
        "    c = (2 * f - 1 - f % 2) / (2.0 * f)\n",
        "    bilinear = np.zeros([f_shape[0], f_shape[1]])\n",
        "    for x in range(width):\n",
        "        for y in range(heigh):\n",
        "            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n",
        "            bilinear[x, y] = value\n",
        "    weights = np.zeros(f_shape)\n",
        "    for i in range(f_shape[2]):\n",
        "        weights[:, :, i, i] = bilinear\n",
        "\n",
        "    init = tf.constant_initializer(value=weights,\n",
        "                                   dtype=tf.float32)\n",
        "    var = tf.get_variable(name=\"up_filter\", initializer=init,\n",
        "                          shape=weights.shape)\n",
        "    return var\n",
        "\n",
        "def upscore_layer(x, shape, num_classes, name, ksize, stride):\n",
        "    strides = [1, stride, stride, 1]\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "        in_features = x.get_shape()[3].value\n",
        "        if shape is None:\n",
        "            in_shape = tf.shape(x)\n",
        "            h = ((in_shape[1] - 1) * stride) + 1\n",
        "            w = ((in_shape[2] - 1) * stride) + 1\n",
        "            new_shape = [in_shape[0], h, w, num_classes]\n",
        "        else:\n",
        "            new_shape = [shape[0], shape[1], shape[2], num_classes]\n",
        "        output_shape = tf.stack(new_shape)\n",
        "        f_shape = [ksize, ksize, num_classes, in_features]\n",
        "        num_input = ksize * ksize * in_features / stride\n",
        "        stddev = (2 / num_input)**0.5\n",
        "        weights = get_deconv_filter(f_shape)\n",
        "        deconv = tf.nn.conv2d_transpose(x, weights, output_shape, strides = strides, padding='SAME')\n",
        "        return deconv\n",
        "\n",
        "def score_layer(x, name, num_classes, stddev = 0.001): \n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE) as scope:\n",
        "        # get number of input channels\n",
        "        in_features = x.get_shape()[3].value\n",
        "        shape = [1, 1, in_features, num_classes]\n",
        "        w_decay = 5e-4\n",
        "        init = tf.truncated_normal_initializer(stddev = stddev)\n",
        "        weights = tf.get_variable(\"weights\", shape = shape, initializer = init)\n",
        "        collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n",
        "\n",
        "        if not tf.get_variable_scope().reuse:\n",
        "            weight_decay = tf.multiply(tf.nn.l2_loss(weights), w_decay, name='weight_loss')\n",
        "            tf.add_to_collection(collection_name, weight_decay)\n",
        "\n",
        "        conv = tf.nn.conv2d(x, weights, [1, 1, 1, 1], padding='SAME')\n",
        "        # Apply bias\n",
        "        initializer = tf.constant_initializer(0.0)\n",
        "        conv_biases = tf.get_variable(name='biases', shape=[num_classes],initializer=initializer)\n",
        "\n",
        "        bias = tf.nn.bias_add(conv, conv_biases)\n",
        "\n",
        "        return bias     \n",
        "      \n",
        "\n",
        "def inference(features, labels, params):\n",
        "  \n",
        "  num_classes=params['num_classes']\n",
        "  \n",
        "  with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
        "    net, end_points = resnet_v1.resnet_v1_50(features,\n",
        "                                             global_pool=False, \n",
        "                                             output_stride=32)\n",
        "    \n",
        "    scale5 = net\n",
        "    scale4 = end_points['resnet_v1_50/block3/unit_5/bottleneck_v1']\n",
        "    scale3 = end_points['resnet_v1_50/block2/unit_3/bottleneck_v1']\n",
        "    scale2 = end_points['resnet_v1_50/block1/unit_2/bottleneck_v1']\n",
        "    input_x = features\n",
        "            \n",
        "      # stride=32\n",
        "      #resnet_v1_50/block3/unit_5/bottleneck_v1 (1, 15, 15, 1024)\n",
        "      #net.shape: (1, 8, 8, 2048)  \n",
        "      #resnet_v1_50/block2/unit_3/bottleneck_v1 (1, 29, 29, 512)\n",
        "      #resnet_v1_50/block1/unit_2/bottleneck_v1 (1, 57, 57, 256)\n",
        "      #resnet_v1_50/conv1 (1, 113, 113, 64)\n",
        "      \n",
        "  with tf.variable_scope('scale_fcn'):\n",
        "    upscore2 = upscore_layer(scale5, shape = tf.shape(scale4), num_classes = num_classes, name = \"upscore2\", ksize = 4, stride = 2) \n",
        "    score_scale4 = score_layer(scale4, \"score_scale4\", num_classes = num_classes)\n",
        "    fuse_scale4 = tf.add(upscore2, score_scale4)\n",
        "      \n",
        "    upscore4 = upscore_layer(fuse_scale4, shape = tf.shape(scale3), num_classes = num_classes, name = \"upscore4\", ksize = 4, stride = 2) \n",
        "    score_scale3 = score_layer(scale3, \"score_scale3\", num_classes = num_classes)\n",
        "    fuse_scale3 = tf.add(upscore4, score_scale3)\n",
        "      \n",
        "    upscore8 = upscore_layer(fuse_scale3, shape = tf.shape(scale2), num_classes = num_classes, name = \"upscore8\", ksize = 4, stride = 2) \n",
        "    #score_scale2 = score_layer(scale2, \"score_scale2\", num_classes = num_classes)\n",
        "    #fuse_scale2 = tf.add(upscore8, score_scale2)\n",
        "    #upscore32 = upscore_layer(fuse_scale2, shape = tf.shape(input_x), num_classes = num_classes, name = \"upscore32\", ksize = 8, stride = 4)\n",
        "      \n",
        "    upscore32 = upscore_layer(upscore8, shape = tf.shape(input_x), num_classes = num_classes, name = \"upscore32\", ksize = 8, stride = 4)\n",
        "    pred_up = tf.argmax(upscore32, dimension = 3)\n",
        "    pred = tf.expand_dims(pred_up, dim = 3, name='pred')     \n",
        "      \n",
        "  # Using tf.losses, any loss is added to the tf.GraphKeys.LOSSES collection\n",
        "  # We can then call the total loss easily    \n",
        "  tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=upscore32)\n",
        "  loss = tf.losses.get_total_loss()\n",
        "      \n",
        "  # Compute evaluation metrics.\n",
        "  accuracy = tf.metrics.mean_iou(labels=labels,\n",
        "                                     num_classes=2,\n",
        "                                     predictions=pred_up,\n",
        "                                     name=\"acc_op\")\n",
        "      \n",
        "  tf.summary.scalar('accuracy', accuracy[0])\n",
        "  \n",
        "  global_step = tf.train.get_or_create_global_step()     \n",
        "      \n",
        "  # set initial learning rate and \n",
        "  # scale it by 0.1 in every 3000 steps\n",
        "  boundaries = [i*params['decay_steps'] for i in range(1, int(np.ceil(params['max_steps']/params['decay_steps']))) ]\n",
        "  boundaries[-1] = params['max_steps']\n",
        "  initial = [params['initial_learning_rate']]\n",
        "  values = initial + [params['initial_learning_rate']*(0.1**i) for i in range(1,len(boundaries)+1)]\n",
        "  lr = tf.train.piecewise_constant(global_step, boundaries, values)\n",
        "\n",
        "\n",
        "  # Variables that affect learning rate\n",
        "  var_resnet_batchnorm = [var for var in tf.trainable_variables() if ('conv1/BatchNorm' in var.name or 'conv2/BatchNorm' in var.name or 'conv3/BatchNorm' in var.name)] \n",
        "  \n",
        "  var_upscale = [var for var in tf.trainable_variables() if 'up_filter' in var.name]\n",
        "  \n",
        "  var_rest = [var for var in tf.trainable_variables() if var.name not in var_resnet_batchnorm+var_upscale]\n",
        "  \n",
        "  opt1 = tf.train.GradientDescentOptimizer(0)\n",
        "  opt2 = tf.train.GradientDescentOptimizer(lr*0.1)\n",
        "  opt3 = tf.train.GradientDescentOptimizer(lr)\n",
        "  \n",
        "  grads = tf.gradients(loss, var_resnet_batchnorm + var_upscale + var_rest)\n",
        "  \n",
        "  grads1 = grads[:len(var_resnet_batchnorm)]\n",
        "  grads2 = grads[len(var_resnet_batchnorm):len(var_resnet_batchnorm)+len(var_upscale)]\n",
        "  grads3 = grads[len(var_resnet_batchnorm)+len(var_upscale):]\n",
        "  \n",
        "  train_op1 = opt1.apply_gradients(zip(grads1, var_resnet_batchnorm), global_step=global_step)\n",
        "  train_op2 = opt2.apply_gradients(zip(grads2, var_upscale), global_step=global_step)\n",
        "  train_op3 = opt3.apply_gradients(zip(grads3, var_rest), global_step=global_step)\n",
        "  \n",
        "  train_op = tf.group(train_op1, train_op2, train_op3)\n",
        "\n",
        "  #decay_steps=10000\n",
        "  #lr = tf.train.exponential_decay(params['initial_learning_rate'],\n",
        "  #                               global_step,\n",
        "  #                              decay_steps,\n",
        "  #                             params['learning_rate_decay_factor'],\n",
        "  #                            staircase=True)\n",
        "  \n",
        "  #train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss, global_step=global_step) \n",
        "      \n",
        "  return loss, accuracy[0], train_op\n",
        "      \n",
        "\n",
        "filename = 'melanoma_train.tfrecords'\n",
        "batch_size=1\n",
        "epochs=10\n",
        "save_model_path='./checkpoints'\n",
        "restore_ckpt_path='./resnet_v1_50.ckpt'\n",
        "initial_learning_rate=1e-3\n",
        "num_classes=2\n",
        "params = {'initial_learning_rate' : initial_learning_rate,\n",
        "          'num_classes' : num_classes,\n",
        "          'decay_steps' : 3000,\n",
        "          'max_steps' : 100000}\n",
        "  \n",
        "  \n",
        "features_op, labels_op = my_input_fn(filename, batch_size, epochs) \n",
        "ops = inference(features_op, labels_op, params)\n",
        "  \n",
        "  \n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  exclude_restore = [var.name for var in tf.global_variables() if ('logits' in var.name or 'scale_fcn' in var.name) ] \n",
        "  \n",
        "  \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.local_variables_initializer())\n",
        "  \n",
        "  variables_to_restore = slim.get_variables_to_restore(exclude=exclude_restore)\n",
        "  tf.train.init_from_checkpoint(restore_ckpt_path,\n",
        "                                {v.name.split(':')[0]: v for v in variables_to_restore})\n",
        "  \n",
        "  saver = tf.train.Saver()\n",
        "  \n",
        "  train_step = 0\n",
        "  print_every = 100\n",
        "  while True:\n",
        "    try:\n",
        "      \n",
        "      loss, acc, _ = sess.run(ops)\n",
        "      \n",
        "      train_step += 1\n",
        "      \n",
        "      print('training loss: {},  step: {}'.format(loss, train_step))\n",
        "      \n",
        "      if train_step % print_every == 0:\n",
        "        print('training mIOU: {},  step: {}'.format(acc, train_step))\n",
        "      \n",
        "      \n",
        "    except tf.errors.OutOfRangeError:\n",
        "      \n",
        "      print('Training finished. Saving the resulting model to {}'.format(save_model_path))\n",
        "      save_path = saver.save(sess, args.save_path)\n",
        "      \n",
        "      break  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}